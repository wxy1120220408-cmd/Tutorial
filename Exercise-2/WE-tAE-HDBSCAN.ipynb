{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e92d02-6317-4a42-b046-1b5c5f5ea90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from deeptime.util.torch import MLP\n",
    "from deeptime.decomposition.deep import TAE\n",
    "from deeptime.util.data import TrajectoryDataset\n",
    "import hdbscan\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ea3c8-8a3b-4bf5-b711-9e04faffe7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization Settings\n",
    "# ============================================================\n",
    "mpl.rcParams['axes.linewidth']    = 2\n",
    "mpl.rcParams['xtick.major.size']  = 4\n",
    "mpl.rcParams['xtick.major.width'] = 2\n",
    "mpl.rcParams['ytick.major.size']  = 4\n",
    "mpl.rcParams['ytick.major.width'] = 2\n",
    "mpl.rcParams['xtick.direction']   = 'in'\n",
    "mpl.rcParams['ytick.direction']   = 'in'\n",
    "mpl.rcParams['font.size']         = 14\n",
    "mpl.rcParams['savefig.dpi']       = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08c186-1e55-445d-82fd-ae230bb1a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1: Load Data & Create Output Directories\n",
    "# ============================================================\n",
    "os.makedirs('wt3-150/wt3-tae/tae-new/wt3-60', exist_ok=True)\n",
    "os.makedirs('wt3-150/wt3-tae/tae-new/orin-60', exist_ok=True)\n",
    "os.makedirs('tae-noise', exist_ok=True)\n",
    "\n",
    "loaded = np.load('tri_sin_phi_data.npz')\n",
    "tri_sin_phi = loaded['tri_sin_phi']\n",
    "\n",
    "# origin data\n",
    "data_orig = tri_sin_phi[:, 1:7]\n",
    "\n",
    "# Data after wavelet denoising (generated by WE-tICA-HDBSCAN)\n",
    "data_wt = np.loadtxt('tri_wt3_data.txt')\n",
    "\n",
    "# dihedral angle data\n",
    "phi1 = tri_sin_phi[:, 7]\n",
    "phi2 = tri_sin_phi[:, 8]\n",
    "phi3 = tri_sin_phi[:, 9]\n",
    "\n",
    "print(f\"data_orig shape:    {data_orig.shape}\")\n",
    "print(f\"data_wt shape:      {data_wt.shape}\")\n",
    "print(f\"tri_sin_phi shape:  {tri_sin_phi.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a8435-10b7-4464-9178-e61944f697db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2a: Prepare DataLoader for TAE Training (Wavelet)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Preparing Wavelet Data for TAE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lag_time = 60\n",
    "dataset_wt = TrajectoryDataset(lag_time, data_wt.astype(np.float32))\n",
    "\n",
    "n_val_wt = int(len(dataset_wt) * 0.4)\n",
    "train_data_wt, val_data_wt = torch.utils.data.random_split(\n",
    "    dataset_wt, [len(dataset_wt) - n_val_wt, n_val_wt]\n",
    ")\n",
    "\n",
    "loader_train_wt = DataLoader(train_data_wt, batch_size=32, shuffle=True)\n",
    "loader_val_wt   = DataLoader(val_data_wt,   batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Wavelet - Training samples:   {len(train_data_wt)}\")\n",
    "print(f\"Wavelet - Validation samples: {len(val_data_wt)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c573d40-1dbe-4b29-b1ba-a829223cc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2b: Prepare DataLoader for TAE Training (Original)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Preparing Original Data for TAE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dataset_orig = TrajectoryDataset(lag_time, data_orig.astype(np.float32))\n",
    "\n",
    "n_val_orig = int(len(dataset_orig) * 0.4)\n",
    "train_data_orig, val_data_orig = torch.utils.data.random_split(\n",
    "    dataset_orig, [len(dataset_orig) - n_val_orig, n_val_orig]\n",
    ")\n",
    "\n",
    "loader_train_orig = DataLoader(train_data_orig, batch_size=32, shuffle=True)\n",
    "loader_val_orig   = DataLoader(val_data_orig,   batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Original - Training samples:   {len(train_data_orig)}\")\n",
    "print(f\"Original - Validation samples: {len(val_data_orig)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410712b-d2d1-49b5-923c-1f9d158f0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3: Define TAE Model Architecture\n",
    "#   encoder: 6 -> 12 -> 12 -> 2 (with Tanh activation)\n",
    "#   decoder: 2 -> 12 -> 12 -> 6 (reverse of encoder)\n",
    "# ============================================================\n",
    "units = [6, 12, 12, 2]\n",
    "\n",
    "def create_tae_model():\n",
    "    encoder = MLP(\n",
    "        units,\n",
    "        nonlinearity=torch.nn.Tanh,\n",
    "        output_nonlinearity=torch.nn.Tanh,\n",
    "        initial_batchnorm=False\n",
    "    )\n",
    "    decoder = MLP(\n",
    "        units[::-1],  # reverse: [2, 12, 12, 6]\n",
    "        nonlinearity=torch.nn.Tanh,\n",
    "        initial_batchnorm=False\n",
    "    )\n",
    "    return TAE(encoder, decoder, optimizer='Adam', learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98213a6-ea32-4869-8da8-f7970e4955c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4a: Train TAE Model (Wavelet)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training TAE Model - Wavelet Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tae_wt = create_tae_model()\n",
    "tae_wt.fit(loader_train_wt, validation_loader=loader_val_wt, n_epochs=100)\n",
    "print(\"Wavelet TAE training completed.\")\n",
    "\n",
    "# --- plot training & validation loss ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(*tae_wt.train_losses.T, label='Train Loss (Wavelet)')\n",
    "plt.semilogy(*tae_wt.validation_losses.T, label='Validation Loss (Wavelet)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('TAE Training & Validation Loss (Wavelet)')\n",
    "plt.savefig('tae_training_loss_wavelet.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- save training losses ---\n",
    "np.save('wt3-150/wt3-tae/tae-new/wt3-60/train_losses.npy', np.array(tae_wt.train_losses))\n",
    "np.save('wt3-150/wt3-tae/tae-new/wt3-60/val_losses.npy', np.array(tae_wt.validation_losses))\n",
    "\n",
    "# --- save model ---\n",
    "tae_model_wt = tae_wt.fetch_model()\n",
    "torch.save(tae_model_wt, 'wt3-150/wt3-tae/tae-new/wt3-60/tae_model.pth')\n",
    "print(\"Wavelet model saved to: wt3-150/wt3-tae/tae-new/wt3-60/tae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4bdeb-207c-451f-b55a-a6f3b379a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4b: Train TAE Model (Original)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training TAE Model - Original Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tae_orig = create_tae_model()\n",
    "tae_orig.fit(loader_train_orig, validation_loader=loader_val_orig, n_epochs=100)\n",
    "print(\"Original TAE training completed.\")\n",
    "\n",
    "# --- plot training & validation loss ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(*tae_orig.train_losses.T, label='Train Loss (Original)')\n",
    "plt.semilogy(*tae_orig.validation_losses.T, label='Validation Loss (Original)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('TAE Training & Validation Loss (Original)')\n",
    "plt.savefig('tae_training_loss_original.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- save training losses ---\n",
    "np.save('wt3-150/wt3-tae/tae-new/orin-60/train_losses.npy', np.array(tae_orig.train_losses))\n",
    "np.save('wt3-150/wt3-tae/tae-new/orin-60/val_losses.npy', np.array(tae_orig.validation_losses))\n",
    "\n",
    "# --- save model ---\n",
    "tae_model_orig = tae_orig.fetch_model()\n",
    "torch.save(tae_model_orig, 'wt3-150/wt3-tae/tae-new/orin-60/tae_model.pth')\n",
    "print(\"Original model saved to: wt3-150/wt3-tae/tae-new/orin-60/tae_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e32ce-48d0-4d01-9b84-fd54ce091184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5: Transform Data to TAE Latent Space\n",
    "# ============================================================\n",
    "# --- wavelet ---\n",
    "dcv_wt = tae_model_wt.transform(data_wt)\n",
    "print(f\"\\nWavelet TAE latent space shape: {dcv_wt.shape}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(dcv_wt[:, 0], dcv_wt[:, 1], s=0.1, alpha=0.1)\n",
    "plt.xlabel('dCV1', fontsize=16)\n",
    "plt.ylabel('dCV2', fontsize=16)\n",
    "plt.title('TAE Latent Space (Wavelet)', fontsize=16, fontweight='bold')\n",
    "plt.savefig('tae_latent_space_wavelet.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- original ---\n",
    "dcv_orig = tae_model_orig.transform(data_orig)\n",
    "print(f\"Original TAE latent space shape: {dcv_orig.shape}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(dcv_orig[:, 0], dcv_orig[:, 1], s=0.1, alpha=0.1)\n",
    "plt.xlabel('dCV1', fontsize=16)\n",
    "plt.ylabel('dCV2', fontsize=16)\n",
    "plt.title('TAE Latent Space (Original)', fontsize=16, fontweight='bold')\n",
    "plt.savefig('tae_latent_space_original.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0acf82c-fbd1-400c-b2a1-13ccceb7e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6a: HDBSCAN Clustering on TAE Latent Space (Wavelet)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HDBSCAN Clustering - Wavelet TAE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cluster_wt = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=600,\n",
    "    min_samples=100,\n",
    "    core_dist_n_jobs=6,\n",
    "    cluster_selection_method='eom',\n",
    "    gen_min_span_tree=True\n",
    ")\n",
    "cluster_wt.fit(dcv_wt)\n",
    "\n",
    "labels_wt = cluster_wt.labels_\n",
    "mask_wt = labels_wt != -1\n",
    "\n",
    "\n",
    "n_clusters_wt = len(set(labels_wt)) - (1 if -1 in labels_wt else 0)\n",
    "n_noise_wt = np.sum(labels_wt == -1)\n",
    "print(f\"Total points:          {len(labels_wt)}\")\n",
    "print(f\"Noise points (-1):     {n_noise_wt} ({n_noise_wt/len(labels_wt)*100:.2f}%)\")\n",
    "print(f\"Clustered points:      {np.sum(mask_wt)} ({np.sum(mask_wt)/len(labels_wt)*100:.2f}%)\")\n",
    "print(f\"Number of clusters:    {n_clusters_wt}\")\n",
    "for k in set(labels_wt):\n",
    "    if k != -1:\n",
    "        count = np.sum(labels_wt == k)\n",
    "        print(f\"  Cluster {k}: {count} points ({count/len(labels_wt)*100:.2f}%)\")\n",
    "\n",
    "# --- plot: Wavelet TAE latent space with clustering ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Noise point (gray)\n",
    "plt.scatter(dcv_wt[~mask_wt, 0], dcv_wt[~mask_wt, 1],\n",
    "            c='lightgray', s=1, alpha=0.3, label='Noise')\n",
    "# Cluster points (color)\n",
    "if mask_wt.sum() > 0:\n",
    "    plt.scatter(dcv_wt[mask_wt, 0], dcv_wt[mask_wt, 1],\n",
    "                c=labels_wt[mask_wt], s=1, alpha=0.8, cmap='tab20')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "plt.xlabel('dCV1', fontsize=16)\n",
    "plt.ylabel('dCV2', fontsize=16)\n",
    "plt.title(f'Wavelet TAE + HDBSCAN (n_clusters={n_clusters_wt})', fontsize=16, fontweight='bold')\n",
    "plt.savefig('tae_hdbscan_clustering_wavelet.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6baf62a-af87-495d-8397-f1473fe35435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6b: HDBSCAN Clustering on TAE Latent Space (Original)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HDBSCAN Clustering - Original TAE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cluster_orig = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=600,\n",
    "    min_samples=100,\n",
    "    core_dist_n_jobs=6,\n",
    "    cluster_selection_method='eom',\n",
    "    gen_min_span_tree=True\n",
    ")\n",
    "cluster_orig.fit(dcv_orig)\n",
    "\n",
    "labels_orig = cluster_orig.labels_\n",
    "mask_orig = labels_orig != -1\n",
    "\n",
    "\n",
    "n_clusters_orig = len(set(labels_orig)) - (1 if -1 in labels_orig else 0)\n",
    "n_noise_orig = np.sum(labels_orig == -1)\n",
    "print(f\"Total points:          {len(labels_orig)}\")\n",
    "print(f\"Noise points (-1):     {n_noise_orig} ({n_noise_orig/len(labels_orig)*100:.2f}%)\")\n",
    "print(f\"Clustered points:      {np.sum(mask_orig)} ({np.sum(mask_orig)/len(labels_orig)*100:.2f}%)\")\n",
    "print(f\"Number of clusters:    {n_clusters_orig}\")\n",
    "for k in set(labels_orig):\n",
    "    if k != -1:\n",
    "        count = np.sum(labels_orig == k)\n",
    "        print(f\"  Cluster {k}: {count} points ({count/len(labels_orig)*100:.2f}%)\")\n",
    "\n",
    "# --- plot: Original TAE latent space with clustering ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Noise point (gray)\n",
    "plt.scatter(dcv_orig[~mask_orig, 0], dcv_orig[~mask_orig, 1],\n",
    "            c='lightgray', s=1, alpha=0.3, label='Noise')\n",
    "# Cluster points (color)\n",
    "if mask_orig.sum() > 0:\n",
    "    plt.scatter(dcv_orig[mask_orig, 0], dcv_orig[mask_orig, 1],\n",
    "                c=labels_orig[mask_orig], s=1, alpha=0.8, cmap='tab20')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "plt.xlabel('dCV1', fontsize=16)\n",
    "plt.ylabel('dCV2', fontsize=16)\n",
    "plt.title(f'Original TAE + HDBSCAN (n_clusters={n_clusters_orig})', fontsize=16, fontweight='bold')\n",
    "plt.savefig('tae_hdbscan_clustering_original.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10cb6f2-9306-4dfc-8b5b-486ea4499a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 7: Project Clustering Results onto Dihedral Angle Space\n",
    "# ============================================================\n",
    "# --- 7a: Wavelet ---\n",
    "print(\"\\nPlotting Wavelet TAE clusters in phi space...\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# phi1 vs phi2\n",
    "ax = axes[0]\n",
    "ax.scatter(phi1[~mask_wt], phi2[~mask_wt], c='lightgray', s=1, alpha=0.3)\n",
    "if mask_wt.sum() > 0:\n",
    "    ax.scatter(phi1[mask_wt], phi2[mask_wt], c=labels_wt[mask_wt], s=1, alpha=0.8, cmap='tab20')\n",
    "ax.set_xlabel('φ1 (degrees)', fontsize=16)\n",
    "ax.set_ylabel('φ2 (degrees)', fontsize=16)\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_ylim(-180, 180)\n",
    "ax.set_title('φ1 vs φ2', fontsize=14, fontweight='bold')\n",
    "\n",
    "# phi1 vs phi3\n",
    "ax = axes[1]\n",
    "ax.scatter(phi1[~mask_wt], phi3[~mask_wt], c='lightgray', s=1, alpha=0.3)\n",
    "if mask_wt.sum() > 0:\n",
    "    ax.scatter(phi1[mask_wt], phi3[mask_wt], c=labels_wt[mask_wt], s=1, alpha=0.8, cmap='tab20')\n",
    "ax.set_xlabel('φ1 (degrees)', fontsize=16)\n",
    "ax.set_ylabel('φ3 (degrees)', fontsize=16)\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_ylim(-180, 180)\n",
    "ax.set_title('φ1 vs φ3', fontsize=14, fontweight='bold')\n",
    "\n",
    "# phi2 vs phi3\n",
    "ax = axes[2]\n",
    "ax.scatter(phi2[~mask_wt], phi3[~mask_wt], c='lightgray', s=1, alpha=0.3)\n",
    "if mask_wt.sum() > 0:\n",
    "    sc = ax.scatter(phi2[mask_wt], phi3[mask_wt], c=labels_wt[mask_wt], s=1, alpha=0.8, cmap='tab20')\n",
    "    plt.colorbar(sc, ax=ax, label='Cluster Label')\n",
    "ax.set_xlabel('φ2 (degrees)', fontsize=16)\n",
    "ax.set_ylabel('φ3 (degrees)', fontsize=16)\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_ylim(-180, 180)\n",
    "ax.set_title('φ2 vs φ3', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Wavelet TAE Clustering in Dihedral Angle Space', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tae_hdbscan_phi_space_wavelet.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- 7b: Original ---\n",
    "print(\"Plotting Original TAE clusters in phi space...\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# phi1 vs phi2\n",
    "ax = axes[0]\n",
    "ax.scatter(phi1[~mask_orig], phi2[~mask_orig], c='lightgray', s=1, alpha=0.3)\n",
    "if mask_orig.sum() > 0:\n",
    "    ax.scatter(phi1[mask_orig], phi2[mask_orig], c=labels_orig[mask_orig], s=1, alpha=0.8, cmap='tab20')\n",
    "ax.set_xlabel('φ1 (degrees)', fontsize=16)\n",
    "ax.set_ylabel('φ2 (degrees)', fontsize=16)\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_ylim(-180, 180)\n",
    "ax.set_title('φ1 vs φ2', fontsize=14, fontweight='bold')\n",
    "\n",
    "# phi1 vs phi3\n",
    "ax = axes[1]\n",
    "ax.scatter(phi1[~mask_orig], phi3[~mask_orig], c='lightgray', s=1, alpha=0.3)\n",
    "if mask_orig.sum() > 0:\n",
    "    ax.scatter(phi1[mask_orig], phi3[mask_orig], c=labels_orig[mask_orig], s=1, alpha=0.8, cmap='tab20')\n",
    "ax.set_xlabel('φ1 (degrees)', fontsize=16)\n",
    "ax.set_ylabel('φ3 (degrees)', fontsize=16)\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_ylim(-180, 180)\n",
    "ax.set_title('φ1 vs φ3', fontsize=14, fontweight='bold')\n",
    "\n",
    "# phi2 vs phi3\n",
    "ax = axes[2]\n",
    "ax.scatter(phi2[~mask_orig], phi3[~mask_orig], c='lightgray', s=1, alpha=0.3)\n",
    "if mask_orig.sum() > 0:\n",
    "    sc = ax.scatter(phi2[mask_orig], phi3[mask_orig], c=labels_orig[mask_orig], s=1, alpha=0.8, cmap='tab20')\n",
    "    plt.colorbar(sc, ax=ax, label='Cluster Label')\n",
    "ax.set_xlabel('φ2 (degrees)', fontsize=16)\n",
    "ax.set_ylabel('φ3 (degrees)', fontsize=16)\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_ylim(-180, 180)\n",
    "ax.set_title('φ2 vs φ3', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Original TAE Clustering in Dihedral Angle Space', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tae_hdbscan_phi_space_original.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec0a43-db7c-4455-872f-0cbc0189702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 8: Save Clustering Results\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Clustering Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Wavelet TAE results ---\n",
    "# TAE space\n",
    "np.savetxt('tae-noise/wt3_noise_tae.txt',\n",
    "           np.column_stack((dcv_wt[~mask_wt, 0], dcv_wt[~mask_wt, 1])),\n",
    "           delimiter='\\t', header='dCV1\\tdCV2', comments='')\n",
    "\n",
    "np.savetxt('tae-noise/wt3_clusters_tae.txt',\n",
    "           np.column_stack((dcv_wt[mask_wt, 0], dcv_wt[mask_wt, 1], labels_wt[mask_wt])),\n",
    "           delimiter='\\t', header='dCV1\\tdCV2\\tCluster_Label', comments='')\n",
    "\n",
    "np.savetxt('wt3-150/wt3-tae/tae-new/wt3-60/wt3_cluster-tae-60.txt',\n",
    "           np.column_stack((dcv_wt[mask_wt, 0], dcv_wt[mask_wt, 1], labels_wt[mask_wt])),\n",
    "           delimiter='\\t', comments='')\n",
    "\n",
    "# phi space\n",
    "np.savetxt('tae-noise/wt3_noise_phi23.txt',\n",
    "           np.column_stack((phi2[~mask_wt], phi3[~mask_wt])),\n",
    "           delimiter='\\t', header='phi2\\tphi3', comments='')\n",
    "\n",
    "np.savetxt('tae-noise/wt3_clusters_phi23.txt',\n",
    "           np.column_stack((phi2[mask_wt], phi3[mask_wt], labels_wt[mask_wt])),\n",
    "           delimiter='\\t', header='phi2\\tphi3\\tCluster_Label', comments='')\n",
    "\n",
    "np.savetxt('wt3-150/wt3-tae/tae-new/wt3-60/tri-wt3-cluster-phi13.txt',\n",
    "           np.column_stack((phi1[mask_wt], phi3[mask_wt], labels_wt[mask_wt])),\n",
    "           delimiter='\\t', comments='')\n",
    "\n",
    "print(\"Wavelet TAE clustering results saved.\")\n",
    "\n",
    "# --- Original TAE results ---\n",
    "# TAE space\n",
    "np.savetxt('tae-noise/orin_noise_tae.txt',\n",
    "           np.column_stack((dcv_orig[~mask_orig, 0], dcv_orig[~mask_orig, 1])),\n",
    "           delimiter='\\t', header='dCV1\\tdCV2', comments='')\n",
    "\n",
    "np.savetxt('tae-noise/orin_clusters_tae.txt',\n",
    "           np.column_stack((dcv_orig[mask_orig, 0], dcv_orig[mask_orig, 1], labels_orig[mask_orig])),\n",
    "           delimiter='\\t', header='dCV1\\tdCV2\\tCluster_Label', comments='')\n",
    "\n",
    "np.savetxt('wt3-150/wt3-tae/tae-new/orin-60/orin_cluster-tae-60.txt',\n",
    "           np.column_stack((dcv_orig[mask_orig, 0], dcv_orig[mask_orig, 1], labels_orig[mask_orig])),\n",
    "           delimiter='\\t', comments='')\n",
    "\n",
    "# phi space\n",
    "np.savetxt('tae-noise/orin_noise_phi23.txt',\n",
    "           np.column_stack((phi2[~mask_orig], phi3[~mask_orig])),\n",
    "           delimiter='\\t', header='phi2\\tphi3', comments='')\n",
    "\n",
    "np.savetxt('tae-noise/orin_clusters_phi23.txt',\n",
    "           np.column_stack((phi2[mask_orig], phi3[mask_orig], labels_orig[mask_orig])),\n",
    "           delimiter='\\t', header='phi2\\tphi3\\tCluster_Label', comments='')\n",
    "\n",
    "np.savetxt('wt3-150/wt3-tae/tae-new/orin-60/orin_cluster-phi13.txt',\n",
    "           np.column_stack((phi1[mask_orig], phi3[mask_orig], labels_orig[mask_orig])),\n",
    "           delimiter='\\t', comments='')\n",
    "\n",
    "print(\"Original TAE clustering results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f29a5-aff5-4025-b96c-2673e1ad8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 9: Save Model Parameters as Text Files\n",
    "# ============================================================\n",
    "def save_model_parameters(model, save_path, name):\n",
    "    \"\"\"Extract and save all model parameters (weights & biases) as text files\"\"\"\n",
    "    def save_params(module, module_name):\n",
    "        for param_name, param in module.named_parameters():\n",
    "            param_array = param.detach().cpu().numpy()\n",
    "            param_type = 'weights' if 'weight' in param_name else 'biases'\n",
    "            filename = f'{module_name}_{param_name.replace(\".\", \"_\")}_{param_type}.txt'\n",
    "            np.savetxt(os.path.join(save_path, filename), param_array)\n",
    "    \n",
    "    print(f\"\\nExtracting {name} model parameters:\")\n",
    "    save_params(model.encoder, \"encoder\")\n",
    "    save_params(model.decoder, \"decoder\")\n",
    "    print(f\"  Saved to: {save_path}\")\n",
    "\n",
    "save_model_parameters(tae_model_wt, 'wt3-150/wt3-tae/tae-new/wt3-60/', \"Wavelet\")\n",
    "save_model_parameters(tae_model_orig, 'wt3-150/wt3-tae/tae-new/orin-60/', \"Original\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All tasks completed successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da11b40-0815-49ff-b641-55b965d2f9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
